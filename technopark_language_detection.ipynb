{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYRVcJYx8KZd"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing  import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o5xiq3o7i33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3131537d-3b93-4ae0-b17e-f79ec8c70df4"
      },
      "source": [
        "# открываем колабу доступ к гугл-диску, чтобы загрузить датасет\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiyJFJCm8NLU"
      },
      "source": [
        "data_train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/train.csv\")\n",
        "data_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNO4cr0A-lX"
      },
      "source": [
        "Реализуем пайплайн из TfIdfVectorizer и SGDClassifier с логистической функцией потерь. Для улучшения модели используем бэггинг.\n",
        "\n",
        "Так как данных чрезвычайно много, для ускорения вычислений используем только 1/2 датасета, разбив его на три части для реализации бэггинга."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWKg1bqZ8v4b"
      },
      "source": [
        "part = data_train.iloc[:int(data_train.shape[0] / 6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QvaN24GJ7S"
      },
      "source": [
        "x_train = part.values[:,0]\n",
        "label_encoder = LabelEncoder().fit(part.values[:, 1])\n",
        "y_train = label_encoder.transform(part.values[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68hhSseU80OY"
      },
      "source": [
        "pipe = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer(lowercase=True, ngram_range=(3, 5), analyzer='char', min_df=5, max_df=0.5)),\n",
        "    ('model', SGDClassifier(loss='log', class_weight='balanced'))\n",
        "], verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L8gB6_s81n5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafdc536-4133-453a-c64e-9d563092a253"
      },
      "source": [
        "pipe.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total= 3.2min\n",
            "[Pipeline] ............. (step 2 of 2) Processing model, total= 6.1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='char', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=0.5, max_features=None,\n",
              "                                 min_df=5, ngram_range=(3, 5), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...\n",
              "                 SGDClassifier(alpha=0.0001, average=False,\n",
              "                               class_weight='balanced', early_stopping=False,\n",
              "                               epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "                               l1_ratio=0.15, learning_rate='optimal',\n",
              "                               loss='log', max_iter=1000, n_iter_no_change=5,\n",
              "                               n_jobs=None, penalty='l2', power_t=0.5,\n",
              "                               random_state=None, shuffle=True, tol=0.001,\n",
              "                               validation_fraction=0.1, verbose=0,\n",
              "                               warm_start=False))],\n",
              "         verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfNmFvsdF6v1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c488e93d-1f3a-4a5c-880a-52b6ed6493ab"
      },
      "source": [
        "submit_predictions = pipe.predict(data_test.sentence.values)\n",
        "data_test['language'] = label_encoder.classes_[submit_predictions]\n",
        "(data_test[['index', 'language']].to_csv('language_predicted.csv', index=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6SKavc5_nXF"
      },
      "source": [
        "Далее аналогичным образом обучаем модель на второй и третьей 1/6 датасета.\n",
        "\n",
        "Получаем 3 файла предикта - lang1, lang2, lang3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50QVZKYbAhoh"
      },
      "source": [
        "Следующий этап - голосование. Реализуем следующим образом:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKdb4AOm_djF"
      },
      "source": [
        "lang_count = (\n",
        "    data_train.language.value_counts()\n",
        "    .to_frame()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "lang_count.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEkPsOfH_f-8"
      },
      "source": [
        "lang_freq_dict = {}\n",
        "for ind in range(lang_count.shape[0]):\n",
        "    lang_freq_dict[lang_count['index'][ind]] = lang_count['lang1'][ind]/data_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-tmVAuD_hKq"
      },
      "source": [
        "def voting(a, b, c, d):\n",
        "  if (a == b) or (a == c):\n",
        "    return a\n",
        "  elif b == c:\n",
        "    return b\n",
        "  else:\n",
        "    if (d[a] == max(d[a], d[b], d[c])):\n",
        "      return a\n",
        "    elif (d[b] == max(d[a], d[b], d[c])):\n",
        "      return b\n",
        "    else:\n",
        "      return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aADJf2QM_iuR"
      },
      "source": [
        "def summ_vote(df1, df2, df3, d):\n",
        "  df_new = df1.copy()\n",
        "  for i in tqdm_notebook(range(df1.shape[0])):\n",
        "    df_new['language'][i] = voting(df1['language'][i], df2['language'][i], df3['language'][i], d)\n",
        "  return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXmi-tCE_kAw"
      },
      "source": [
        "lang1 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/lang1.csv\")\n",
        "lang2 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/lang2.csv\")\n",
        "lang3 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/lang3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2PVpgfB-4D"
      },
      "source": [
        "final_predict = summ_vote(lang1, lang2, lang3, lang_freq_dict)\n",
        "(final_predict[['index', 'language']].to_csv('predicted.csv', index=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}